












DOING
- x Add word concreteness
- Generate diagrams for all metrics and describe most interesting
- Perform randomized ART, INT and SRH
- Perform word embedding model benchmark

WAITING
- Finish context coverage
- Finish grammar
- Finish word concreteness
- External internal & external similarity
- Redo Haskell

- Normalize term entropy ?
- Semantic sim should be 1 - X


FACTS
1. Download repository information and source code from GitHub.

For our study we included the top 1000 most popular (as expressed in number of
“stars” [likes]) Github projects for each language: C, Clojure, Elixir, Erlang,
Fortran, Haskell, Java, JavaScript, OCaml and Python. We automate the retrieval
of repositories from GitHub by selecting a specified number of the most popular
repositories for each supported programming language, based on the number of
stars they have received using GitHub public API. The framework gathers
repository metadata, including details such as the author, programming language,
and star count. To optimize the download process, we employ Git's sparse
checkou, which allows us to selectively clone only the source code files and
README, minimizing unnecessary data transfer. Repository information is saved in
the PostgreSQL database.

2. Process source code to extract functions and their names. 

After downloading the repositories, the subsequent step involves the extraction
of functions and identifiers from the source code files. This process entails
parsing each file to isolate the functions and identifiers it contains. Included
file extensions are as follows. C: .c; Clojure: .clj; Elixir: .ex, :exs; Erlang:
.erl, .ex, .escript; Fortran: .f, .for, .ftn, .f90, .f95, .f03, .f08, .f15,
.f18, .fpp; Haskell: .hs, .lhs; Java: .java; JavaScript: .js; OCaml: .ml, .mli;
Python: .py.

Each file is parsed using a custom program to extract function names and
identifier names inside function bodies from concrete syntax tree generated by
the tree-sitter parser generator for each supported language. As identifiers, we
count all names inside the function definition including its name. This includes
language keywords, function calls to standard library, preprocessor macros, and
so on. Nested functions are not included in their parents to ensure observations
are not repeated within samples. Here are some examples: <examples>

3. Generate training file to word embedding model and train it

Word embedding model used to calculate semantic similarity was fastText’s CBOW.
('continuous-bag-of-words').  cbow model predicts the target word according to
its context. The context is represented as a bag of the words contained in a
fixed size window around the target word. Let us illustrate this difference with
an example: given the sentence 'Poets have been mysteriously silent on the
subject of cheese' and the target word 'silent', cbow model takes all the words
in a surrounding window, like {been, mysteriously, on, the}, and uses the sum of
their vectors to predict the target. Model was trained on the entire downloaded
codebase which consisted of 9,4 million functions amounting to 256 million
words. To generate a training file, for each function a name set (list of names
separated by whitespace) was added on a separate line. Model with trained with
typical dimension equal 100, with minimal frequency threshold set to 3, learning
rate 0.1, number of epochs 5 and in two wariants: One with window size 5 for
general use and window size 2 to be used as part of word concreteness metric
calculation (explained below).

4. Repository project domain classification.

Table 1 describe each project domain and major software types in each domain.
Repositories were classified manually by one of the authors in three steps:

1. First pass - pick general category.

Initially we had less and more general categories:
Web and networking
Low-level programming
Databases
Computation/Scientific/Math
Machine learning/AI
Educational
Other

But after first pass it turned out that projects are too diverse to be classified
into such general categories. There we extended the list to 12 categories present
in Table 1.

2. Second pass - projects from initial list were classified into more specific
categories.

3. Review pass - all projects were reviewed and corrected where mistakes where detected.

We made the decision to not include projects in categories "Other libraries",
"Other" and "Applications" due to their broadness and heterogeneity.
"Other libraries" category mostly consists of software libraries which don't fit
into categories which are not large enough between all included languages, 
are language-specific, like distributed process management libraries 
for Elixir and Erlang, or they overlap multiple categories and it's unclear
what the best category would be.

"Applications" category mostly consists of complete applications, platforms, systems
or products and therefore we don't expect them to make up a lexically coherent group (each
would have it's own specific domain terms).

5. Function name grammar extraction

Frontend - 
  High-level web development and frameworks, HTML, CSS, static site generators, 
  templating engines, UI components and utilities for web frameworks, DOM,

Backend -
  Network programming, network protocols, backend servers and frameworks,
  network utilities, web crawlers and bots, cloud services and utilities, APIs,
  HTTP clients, web authentication and authorization, web security

Infrastructure, low-level programming
  Operating systems and OS programming, drivers, low-level programming,
  IoT, embedded systems, hardware programming, GPU programming, firmware,
  hacking and reverse engineering utilities

Databases, SQL
  Databases (relational and others), RDMSs, DB drivers and clients, ORMs, 
  query builders, database-related utilities

Educational
  Courses, textbooks, educational and toy implementations, style guides, 
  lists of resources, documentations, tutorials, exercises, examples,
  cheat sheets,

Languages, compilers, DSL
  Languages, DSLs, compilers, compiler modules, compiler utilities,
  interpreters, virtual machines, language runtimes, language parsers and
  lexers,

CLI ?
  CLI and shell utilities, CLI frameworks, CLI parsers, CLI-only applications,
  shell scripting, REPLs, terminal clients and emulators, pretty printers (?)

ML/AI
  Machine learning, LLMs, neural networks, deep learning, 
  AI agents, AI agent frameworks, AI model catalogues, image & sound generation,
  AI API clients and integrations, RAG frameworks, AI-based tools and utilities

Games ?
  Games, game engines, game utilities, game development frameworks, game runtimes,
  graphics engines for games, game physics, game consoles and emulators

Testing
  Testing frameworks, testing utilities, test runners, test libraries,
  mocking, test data generators

Computations, scientific, math
  Scientific computing, math libraries, cryptography, algorithms,
  simulations and modeling, proof assistants, logic solvers,
  statistics,

Project build, tooling, configuration

Code generation, manipulation, analysis

Logging, monitoring
Language syntax and semantics
Data structures
User interface
Applications
Other libraries
Other





To directly compare the effect of Language vs. Domain in a non-parametric,
two-factor scenario, you need a single unified model (one partition of the total
variation). Three broad ways to achieve that:

    Permutation-based two-factor ANOVA (Freedman–Lane, etc.)
    A single-step or fully aligned rank transform approach (an advanced version of ART)
    A robust or rank-based two-way procedure that yields one table (e.g., WRS2 or nparLD in R)

All of these let you see which factor “is more significant” or has the “bigger”
partial sum of squares on a consistent denominator. By contrast, methods that do
partial expansions per factor (classic ART, Scheirer–Ray–Hare, multiple
“aligned(...) for X” columns) will not let you compare Language vs. Domain
directly because they produce separate partial models.