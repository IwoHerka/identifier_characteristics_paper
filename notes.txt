












DOING
- x Add word concreteness
- Generate diagrams for all metrics and describe most interesting
- Perform randomized ART, INT and SRH
- Perform word embedding model benchmark
- Describe methodology
- Describe results and conclusions


WAITING
- Finish context coverage
- Finish grammar
- Finish word concreteness
- External similarity


- Normalize term entropy ?
- Semantic sim should be 1 - X


We will write a methodology section for a research paper with following abstract:

Identifier names convey important semantic cues in source code, and name-based
analyses exploit these cues to detect bugs, predict types, and improve code
readability. Despite the introduction of many lexicon metrics, large-scale,
holistic examinations of code identifiers remain limited. In this work, we
investigate which factor—programming language or project domain—exerts the
greatest influence on identifier characteristics. To address this question, we
implemented automated analysis tool and systematically examined an extensive set
of identifier metrics, including basic properties (length, frequency of
duplicates, casing styles, etc.) and more complex measures (Levenshtein
distance, term entropy, semantic similarity, word concreteness). Empirical study
encompasses 10000 open source projects spanning ten programming languages, three
paradigms (functional, imperative and object-oriented), and multiple project
domains. This multi-faceted approach offers insights into the relative impact of
language, paradigm, and domain on naming practices in open-source code.

We will do this in steps. First, write a short introduction to Methodology
section explainin that in this section outlines the steps taken to collect,
process, and analyze the data and design of the study.

I will now give you collection of facts about first two steps of data
preparation. Don't rewrite this text, start from scratch, but include all of
those facts:

1. Download repository information and source code from GitHub.

For our study we included the top 1000 most popular (as expressed in number of
“stars” [likes]) Github projects for each language: C, Clojure, Elixir, Erlang,
Fortran, Haskell, Java, JavaScript, OCaml and Python. We automate the retrieval
of repositories from GitHub by selecting a specified number of the most popular
repositories for each supported programming language, based on the number of
stars they have received using GitHub public API. The framework gathers
repository metadata, including details such as the author, programming language,
and star count. To optimize the download process, we employ Git's sparse
checkou, which allows us to selectively clone only the source code files and
README, minimizing unnecessary data transfer. Repository information is saved in
the PostgreSQL database.

2. Process source code to extract functions and their names. 

After downloading the repositories, the subsequent step involves the extraction
of functions and identifiers from the source code files. This process entails
parsing each file to isolate the functions and identifiers it contains. Included
file extensions are as follows. C: .c; Clojure: .clj; Elixir: .ex, :exs; Erlang:
.erl, .ex, .escript; Fortran: .f, .for, .ftn, .f90, .f95, .f03, .f08, .f15,
.f18, .fpp; Haskell: .hs, .lhs; Java: .java; JavaScript: .js; OCaml: .ml, .mli;
Python: .py.

Each file is parsed using a custom program to extract function names and
identifier names inside function bodies from concrete syntax tree generated by
the tree-sitter parser generator for each supported language. As identifiers, we
count all names inside the function definition including its name. This includes
language keywords, function calls to standard library, preprocessor macros, and
so on. Nested functions are not included in their parents to ensure observations
are not repeated within samples. Here are some examples: <examples>


I will now give you next section:

Generate training file to word embedding model and train it

Word embedding model used to calculate semantic similarity was fastText’s CBOW.
('continuous-bag-of-words').  cbow model predicts the target word according to
its context. The context is represented as a bag of the words contained in a
fixed size window around the target word. Let us illustrate this difference with
an example: given the sentence 'Poets have been mysteriously silent on the
subject of cheese' and the target word 'silent', cbow model takes all the words
in a surrounding window, like {been, mysteriously, on, the}, and uses the sum of
their vectors to predict the target. Model was trained on the entire downloaded
codebase which consisted of 9,4 million functions amounting to 256 million
words. To generate a training file, for each function a name set (list of names
separated by whitespace) was added on a separate line. Model with trained with
typical dimension equal 100, with minimal frequency threshold set to 3, learning
rate 0.1, number of epochs 5 and in two wariants: One with window size 5 for
general use and window size 2 to be used as part of word concreteness metric
calculation (explained below).


'frontend', 'backend', 'infr', 'edu', 
'db', 'cli', 'lang', 'ml', 'game', 'test', 'comp', 'build', 'code', 'log', 'seman', 'struct', 'ui'

Initial high-level set of domains consisted of:

Web and networking
Low-level programming
Databases
Computation/Scientific/Math
Machine learning/AI
Educational
Other

Project domains included in the study are as follows:

Frontend - 
  High-level web development and frameworks, HTML, CSS, static site generators, 
  templating engines, UI components and utilities for web frameworks, DOM,

Backend -
  Network programming, network protocols, backend servers and frameworks,
  network utilities, web crawlers and bots, cloud services and utilities, APIs,
  HTTP clients, web authentication and authorization, web security

Infrastructure, low-level programming
  Operating systems and OS programming, drivers, low-level programming,
  IoT, embedded systems, hardware programming, GPU programming, firmware,
  hacking and reverse engineering utilities

Databases, SQL
  Databases (relational and others), RDMSs, DB drivers and clients, ORMs, 
  query builders, database-related utilities

Educational
  Courses, textbooks, educational and toy implementations, style guides, 
  lists of resources, documentations, tutorials, exercises, examples,
  cheat sheets,

Languages, compilers, DSL
  Languages, DSLs, compilers, compiler modules, compiler utilities,
  interpreters, virtual machines, language runtimes, language parsers and
  lexers,

CLI ?
  CLI and shell utilities, CLI frameworks, CLI parsers, CLI-only applications,
  shell scripting, REPLs, terminal clients and emulators, pretty printers (?)

ML/AI
  Machine learning, LLMs, neural networks, deep learning, 
  AI agents, AI agent frameworks, AI model catalogues, image & sound generation,
  AI API clients and integrations, RAG frameworks, AI-based tools and utilities

Games ?
  Games, game engines, game utilities, game development frameworks, game runtimes,
  graphics engines for games, game physics, game consoles and emulators

Testing
  Testing frameworks, testing utilities, test runners, test libraries,
  mocking, test data generators

Computations, scientific, math
  Scientific computing, math libraries, cryptography, algorithms,
  simulations and modeling, proof assistants, logic solvers,
  statistics,

Project build, tooling, configuration

Code generation, manipulation, analysis

Logging, monitoring
Language syntax and semantics
Data structures
User interface
Applications
Other libraries
Other

1. First pass - general category
2. Second pass - more specific subcategories
3. Review pass -